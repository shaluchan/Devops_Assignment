
# ğŸ³ DevOps Multi-Stage Web Scraper Assignment

This project demonstrates a multi-stage Docker setup that:
1. Scrapes a website using Node.js + Puppeteer + Chromium (build-time stage)
2. Serves the scraped data using Python + Flask (Python hosting stage)

---

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ Dockerfile          # Multi-stage build file
â”œâ”€â”€ scrape.js           # Node.js script for scraping
â”œâ”€â”€ server.py           # Python Flask app
â”œâ”€â”€ package.json        # Node dependencies
â”œâ”€â”€ requirements.txt    # Python dependencies
â””â”€â”€ README.md           # Documentation
```


## ğŸ§± Build Instructions

### Clone the Repo
```bash
git clone
```

### ğŸ› ï¸ Build the Docker Image with a Website to Scrape

Use the `--build-arg` flag to set the website URL u want to scrape:

```bash
docker build --build-arg SCRAPE_URL="https://anyxyzsite.com" -t web-scraper .
```

---

## ğŸš€ Run the Container

```bash
docker run -p 5000:5000 web-scraper
```

---

## ğŸŒ Access the Scraped Data

Visit this in your browser:
```
http://localhost:5000
```
You will see a JSON output of:
- Page Title
- First Heading (h1, h2, or h3)

---

## ğŸ”„ To Scrape a Different Site
Just rebuild the image with a new URL:
```bash
docker build --build-arg SCRAPE_URL="https://books.toscrape.com" -t web-scraper .
docker run -p 5000:5000 web-scraper
```

---

## âœ… Technologies Used
- Node.js (18-slim)
- Puppeteer + Chromium
- Python (3.10-slim)
- Flask
- Docker Multi-Stage Builds

---

## ğŸ“Œ Notes
- The scraping only happens **at build time**, that's why build args are required.
- The runtime container only hosts the pre-scraped data via Flask.
- This setup keeps the final image lightweight and efficient.
```
